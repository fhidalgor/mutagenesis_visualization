{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This section will teach the different options to normalize the data using the function :func:`mutagenesis_visualization.calculate_enrichment` . \n",
    "If you already have your own processing pipeline built, you can skip this section and go to the (:ref:`Creating plots`) examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:46.376791Z",
     "start_time": "2020-10-01T17:38:46.194343Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from mutagenesis_visualization.main.process_data.count_reads import count_reads\n",
    "from mutagenesis_visualization.main.process_data.count_fastq import count_fastq\n",
    "from mutagenesis_visualization.main.process_data.calculate_enrichment import calculate_enrichment\n",
    "from mutagenesis_visualization.main.demo.demo_data import HRAS_RBD_COUNTS\n",
    "from mutagenesis_visualization.main.kernel.multiple_kernels import MultipleKernel\n",
    "from mutagenesis_visualization.main.classes.screen import Screen\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In here we are loading and assembling the data by hand instead of using the function ``assemble_sublibraries`` found in :ref:`Multiple sublibraries` so you can see how it is done. In case you have stored your data in another format different to us, you can tweak the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:56.716412Z",
     "start_time": "2020-10-01T17:38:47.785754Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of sheets and columns to use\n",
    "sheets_pre: List[str] = ['R1_before', 'R2_before', 'R3_before']\n",
    "sheets_sel: List[str] = ['R1_after', 'R2_after', 'R3_after']\n",
    "columns: List[str] = ['F:BG', 'BH:DK', 'DL:FN']\n",
    "columns_wt: List[str] = ['A', 'B', 'C']\n",
    "\n",
    "# Create dictionary with data. Loading 3 replicates, each of them is divided into 3 pools\n",
    "dict_pre, dict_sel, dict_pre_wt, dict_sel_wt = ({} for i in range(4))\n",
    "\n",
    "# Read counts from file (could be txt, csv, xlsx, etc...)\n",
    "for column, column_wt in zip(columns, columns_wt):\n",
    "    for sheet_pre, sheet_sel in zip(sheets_pre, sheets_sel):\n",
    "        # Pre counts\n",
    "        label_pre = str(sheet_pre + '_' + column_wt)\n",
    "        dict_pre[label_pre] = pd.read_excel(\n",
    "            HRAS_RBD_COUNTS, sheet_pre, skiprows=1, usecols=column, nrows=32\n",
    "        )\n",
    "        # Pre counts wild-type alleles\n",
    "        dict_pre_wt[label_pre] = pd.read_excel(\n",
    "            HRAS_RBD_COUNTS, sheet_pre, usecols=column_wt\n",
    "        )\n",
    "\n",
    "        # Sel counts\n",
    "        label_sel = str(sheet_sel + '_' + column_wt)\n",
    "        dict_sel[label_sel] = pd.read_excel(\n",
    "            HRAS_RBD_COUNTS, sheet_sel, skiprows=1, usecols=column, nrows=32\n",
    "        )\n",
    "        # Sel counts wild-type alleles\n",
    "        dict_sel_wt[label_sel] = pd.read_excel(\n",
    "            HRAS_RBD_COUNTS, sheet_sel, usecols=column_wt\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate log10 enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to calculate the log10(sel/pre) for the sublibrary 1 of each replicate and plot a histogram. The resulting distribution is bimodal, and because the three replicates have a similar number of counts ratios, their center is overlapping. However, because we have not normalized by the number of counts, and there are more counts in the selected than in the pre-selected population, the center is >0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:57.115321Z",
     "start_time": "2020-10-01T17:38:56.718574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Auxiliar function to convert +-inf values to an arbitrary number (ie +-2)\n",
    "def _replace_inf(df: DataFrame) -> DataFrame:\n",
    "    df.replace(to_replace=np.inf, value=2, inplace=True)\n",
    "    df.replace(to_replace=-np.inf, value=-2, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "aminoacids: List[str] = list('AACDEFGGHIKLLLMNPPQRRRSSSTTVVWY*')\n",
    "enrichment = {}\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key in zip(list(dict_pre.keys())[:3],\n",
    "                            list(dict_sel.keys())[:3]):\n",
    "    # log 10\n",
    "    enrichment_log10 = (np.log10(dict_sel[sel_key] / dict_pre[pre_key]))\n",
    "    enrichment_log10['aminoacids'] = aminoacids\n",
    "    enrichment_log10.set_index(['aminoacids'], inplace=True)\n",
    "    enrichment[pre_key[:2]] = _replace_inf(enrichment_log10)\n",
    "\n",
    "plot_multiplekernel(\n",
    "    enrichment,\n",
    "    title='Sublibrary 1, ' + r'$log_{10}$' + '(sel/pre)',\n",
    "    xscale=(-0.5, 0.75),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_kdesub1.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering the data (zeroing)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Functions used in this section:\n",
    "    - :func:`mutagenesis_visualization.plot_multiplekernel`\n",
    "    - :func:`mutagenesis_visualization.calculate_enrichment`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing by the number of counts improves normalization. Now the population center is closer to 0. To do so, set ``zeroing='counts'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:57.375624Z",
     "start_time": "2020-10-01T17:38:57.117223Z"
    }
   },
   "outputs": [],
   "source": [
    "enrichment = {}\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key in zip(list(dict_pre.keys())[:3],\n",
    "                            list(dict_sel.keys())[:3]):\n",
    "    # Enrichment\n",
    "    enrichment_log10 = calculate_enrichment(\n",
    "        dict_pre[pre_key], dict_sel[sel_key], zeroing='counts', stopcodon=False\n",
    "    )\n",
    "    enrichment[pre_key[:2]] = enrichment_log10\n",
    "\n",
    "# Plot histogram and KDE\n",
    "plot_multiplekernel(\n",
    "    enrichment,\n",
    "    title='Sublibrary 1, zeroing = counts',\n",
    "    xscale=(-1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_zeroingcounts.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wt allele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we can normalize is by using an internal reference such as a particular mutant. In the following example we will use the wild-type allele. If the assay that you are using is noisy, relying on a single data point for normalizing will result in high variance. The package does not include this option because it may lead to errors. Here we are showing how it would be done by hand. In this example, it works fine. But in other datasets we have, it has been a source of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:57.535905Z",
     "start_time": "2020-10-01T17:38:57.379429Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate log10 enrichment for each replicate\n",
    "\n",
    "aminoacids: List[str] = list('AACDEFGGHIKLLLMNPPQRRRSSSTTVVWY*')\n",
    "enrichment = {}\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key in zip(list(dict_pre.keys())[:3],\n",
    "                            list(dict_sel.keys())[:3]):\n",
    "    # log 10\n",
    "    wt_ratio = np.log10(\n",
    "        dict_sel_wt[sel_key]['wt 2-56'][1] / dict_pre_wt[pre_key]['wt 2-56'][1]\n",
    "    )\n",
    "    enrichment_log10 = np.log10(\n",
    "        dict_sel[sel_key] / dict_pre[pre_key]\n",
    "    ) - wt_ratio\n",
    "    enrichment_log10['aminoacids'] = aminoacids\n",
    "    enrichment_log10.set_index(['aminoacids'], inplace=True)\n",
    "    enrichment[pre_key[:2]] = _replace_inf(enrichment_log10)\n",
    "\n",
    "plot_multiplekernel(\n",
    "    enrichment,\n",
    "    title='Sublibrary 1, zeroing = wt_allele only',\n",
    "    xscale=(-0.5, 0.5),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_zeroingwtallele.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of synonymous wt alleles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experience, it is better to use the median/mode/mean of the synonymous wild-type population because there is less variance. ``calculate_enrichment`` has such an options by using ``zeroing='wt'`` and then ``how='median', 'mean' or 'mode'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:57.800705Z",
     "start_time": "2020-10-01T17:38:57.537611Z"
    }
   },
   "outputs": [],
   "source": [
    "enrichment = {}\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key in zip(list(dict_pre.keys())[:3],\n",
    "                            list(dict_sel.keys())[:3]):\n",
    "    # Enrichment\n",
    "    enrichment_log10 = calculate_enrichment(\n",
    "        dict_pre[pre_key],\n",
    "        dict_sel[sel_key],\n",
    "        dict_pre_wt[pre_key],\n",
    "        dict_sel_wt[sel_key],\n",
    "        zeroing='wt',\n",
    "        how='mode',\n",
    "        stopcodon=False\n",
    "    )\n",
    "    enrichment[pre_key[:2]] = enrichment_log10\n",
    "\n",
    "# Plot histogram and KDE\n",
    "plot_multiplekernel(\n",
    "    enrichment,\n",
    "    title='Sublibrary 1, zeroing = wt',\n",
    "    xscale=(-1.5, 1),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_zeroingwtpop.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of mutants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative option to normalize the data is to use the mean/median/mode of the population to some specific number such as zero. To do so, use ``zeroing='population'``. The parameters of the distribution will be calculated assuming a gaussian distribution. Not only the three replicates are centered, but also they have the same spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:58.048081Z",
     "start_time": "2020-10-01T17:38:57.802898Z"
    }
   },
   "outputs": [],
   "source": [
    "enrichment = {}\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key in zip(list(dict_pre.keys())[:3],\n",
    "                            list(dict_sel.keys())[:3]):\n",
    "    # Enrichment\n",
    "    enrichment_log10 = calculate_enrichment(\n",
    "        dict_pre[pre_key],\n",
    "        dict_sel[sel_key],\n",
    "        zeroing='population',\n",
    "        how='mode',\n",
    "        stopcodon=False\n",
    "    )\n",
    "    enrichment[pre_key[:2]] = enrichment_log10\n",
    "\n",
    "# Plot histogram and KDE\n",
    "plot_multiplekernel(\n",
    "    enrichment,\n",
    "    title='Sublibrary 1, zeroing = population',\n",
    "    xscale=(-1, 1),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_zeroingpopulation.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T23:42:01.248032Z",
     "start_time": "2020-09-06T23:42:01.028412Z"
    }
   },
   "source": [
    "A variant of the previous method is to calculate the kernel density estimate using ``zeroing='kernel'``. This option centers the population using the mode of the KDE. If the data is bimodal, it will select the main peak. Furthermore, it will use the standard deviation of the main peak to scale the data. This method is useful when you have split your library into multiple pools because it will not only center the data properly but also do scale the data so each pool main peak has the same standard deviation. Results are quite similar to setting ``zeroing='population'`` and ``how='mode'``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:58.899439Z",
     "start_time": "2020-10-01T17:38:58.049996Z"
    }
   },
   "outputs": [],
   "source": [
    "enrichment = {}\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key in zip(list(dict_pre.keys())[:3],\n",
    "                            list(dict_sel.keys())[:3]):\n",
    "    # Enrichment\n",
    "    enrichment_log10 = calculate_enrichment(\n",
    "        dict_pre[pre_key], dict_sel[sel_key], zeroing='kernel', stopcodon=False\n",
    "    )\n",
    "    enrichment[pre_key[:2]] = enrichment_log10\n",
    "\n",
    "# Plot histogram and KDE\n",
    "plot_multiplekernel(\n",
    "    enrichment,\n",
    "    title='Sublibrary 1, zeroing = kernel',\n",
    "    xscale=(-1.5, 1),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_zeroingkernel.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including stop codons in the library can be of great use because it gives a control for basal signal in your assay. The algorithm has the option to apply a baseline subtraction. The way it works is it sets the stop codons counts of the selected population to 0 (baseline) and subtracts the the baseline signal to every other mutant. To use this option, set ``stopcodon=True``. You will notice that it get rids of the shoulder peak, and now the distribution looks unimodal with a big left shoulder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:38:59.616330Z",
     "start_time": "2020-10-01T17:38:58.901569Z"
    }
   },
   "outputs": [],
   "source": [
    "enrichment = {}\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key in zip(list(dict_pre.keys())[:3],\n",
    "                            list(dict_sel.keys())[:3]):\n",
    "    # Enrichment\n",
    "    enrichment_log10 = calculate_enrichment(\n",
    "        dict_pre[pre_key], dict_sel[sel_key], zeroing='kernel', stopcodon=True\n",
    "    )\n",
    "    enrichment[pre_key[:2]] = enrichment_log10\n",
    "\n",
    "# Plot histogram and KDE\n",
    "plot_multiplekernel(\n",
    "    enrichment,\n",
    "    title='Sublibrary 1, baseline subtraction',\n",
    "    xscale=(-5, 1.5),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_baselinesubtr.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you probably have realized that different options of normalization affect to the spread of the data. The rank between each mutant is unchanged between the different methods, so it is a matter of multiplying/dividing by a scalar to adjust the data spread. Changing the value of the parameter ``std_scale`` will do the job. You will probably do some trial an error until you find the right value. In the following example we are changing the ``std_scale`` parameter for each of the three replicates shown. Note that the higher the scalar, the higher the spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:39:00.287209Z",
     "start_time": "2020-10-01T17:38:59.618549Z"
    }
   },
   "outputs": [],
   "source": [
    "enrichment_scalar = {}\n",
    "scalars: List[str] = [0.1, 0.2, 0.3]\n",
    "\n",
    "# calculate log10 enrichment for each replicate\n",
    "for pre_key, sel_key, scalar in zip(list(dict_pre.keys())[:3],\n",
    "                                    list(dict_sel.keys())[:3], scalars):\n",
    "    # Enrichment\n",
    "    enrichment_log10 = calculate_enrichment(\n",
    "        dict_pre[pre_key],\n",
    "        dict_sel[sel_key],\n",
    "        zeroing='kernel',\n",
    "        stopcodon=True,\n",
    "        std_scale=scalar\n",
    "    )\n",
    "    enrichment_scalar[pre_key[:2]] = enrichment_log10\n",
    "\n",
    "# Plot histogram and KDE\n",
    "plot_multiplekernel(\n",
    "    enrichment_scalar,\n",
    "    title='Sublibrary 1, scaling',\n",
    "    xscale=(-5, 1.5),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_scaling.png\n",
    "   :width: 350px\n",
    "   :align: center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple sublibraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our own research projects, where we have multiple DNA pools, we have determined that the combination of parameters that best suit us it to the wild-type synonymous sequences to do a first data normalization step. Then use ``zeroing = 'kernel'`` to zero the data and use ``stopcodon=True`` in order to determine the baseline level of signal. You may need to use different parameters for your purposes. Feel free to get in touch if you have questions regarding data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:39:01.943020Z",
     "start_time": "2020-10-01T17:39:00.289233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels: List[str] = ['Sublibrary 1', 'Sublibrary 2', 'Sublibrary 3']\n",
    "zeroing_options: List[str] = ['population', 'counts', 'wt', 'kernel']\n",
    "title: str = 'Rep-A sublibraries, zeroing = '\n",
    "\n",
    "# xscale\n",
    "xscales = [(-2, 1), (-2.5, 0.5), (-3.5, 1.5), (-3.5, 1.5)]\n",
    "# declare dictionary\n",
    "enrichment_lib = {}\n",
    "df_lib = {}\n",
    "\n",
    "for option, xscale in zip(zeroing_options, xscales):\n",
    "    for pre_key, sel_key, label in zip(list(dict_pre.keys())[::3],\n",
    "                                       list(dict_sel.keys())[::3], labels):\n",
    "        # log 10\n",
    "        enrichment_log10 = calculate_enrichment(\n",
    "            dict_pre[pre_key],\n",
    "            dict_sel[sel_key],\n",
    "            dict_pre_wt[pre_key],\n",
    "            dict_sel_wt[sel_key],\n",
    "            zeroing=option,\n",
    "            how='mode',\n",
    "            stopcodon=True,\n",
    "            infinite=2\n",
    "        )\n",
    "        # Store in dictionary\n",
    "        enrichment_lib[label] = enrichment_log10\n",
    "\n",
    "    # Concatenate sublibraries and store in dict\n",
    "    df = pd.concat([\n",
    "        enrichment_lib['Sublibrary 1'], enrichment_lib['Sublibrary 2'],\n",
    "        enrichment_lib['Sublibrary 3']\n",
    "    ],\n",
    "                   ignore_index=True,\n",
    "                   axis=1)\n",
    "\n",
    "    df_lib[option] = df\n",
    "\n",
    "    # Plot\n",
    "    plot_multiplekernel(\n",
    "        enrichment_lib, title=title + option, xscale=xscale, output_file=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. image:: images/exported_images/hras_repA_zeroingpopulation.png\n",
    "   :width: 350px\n",
    "\n",
    ".. image:: images/exported_images/hras_repA_zeroingcounts.png\n",
    "   :width: 350px\n",
    "   \n",
    ".. image:: images/exported_images/hras_repA_zeroingwt.png\n",
    "   :width: 350px\n",
    "   \n",
    ".. image:: images/exported_images/hras_repA_zeroingkernel.png\n",
    "   :width: 350px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function and class used in this section:\n",
    "    - :class:`mutagenesis_visualization.Screen`\n",
    "    - :meth:`mutagenesis_visualization.heatmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to evaluate how does the heatmap of produced by each of the normalization methods. We are not going to scale the data, so some heatmaps may look more washed out than others. That is not an issue since can easily be changed by using ``std_scale``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:39:02.716897Z",
     "start_time": "2020-10-01T17:39:01.945146Z"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to create the objects\n",
    "\n",
    "# Define protein sequence\n",
    "hras_sequence: str = 'MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLLDILDTAGQEEY'\\\n",
    "                + 'SAMRDQYMRTGEGFLCVFAINNTKSFEDIHQYREQIKRVKDSDDVPMVLVGNKCDLAARTVES'\\\n",
    "                + 'RQAQDLARSYGIPYIETSAKTRQGVEDAFYTLVREIRQHKLRKLNPPDESGPG'\n",
    "\n",
    "# Order of amino acid substitutions in the hras_enrichment dataset\n",
    "aminoacids: List[str] = list('ACDEFGHIKLMNPQRSTVWY*')\n",
    "\n",
    "# First residue of the hras_enrichment dataset. Because 1-Met was not mutated, the dataset starts at residue 2\n",
    "start_position: int = 2\n",
    "\n",
    "# Create objects\n",
    "objects = {}\n",
    "for key, value in df_lib.items():\n",
    "    temp = Screen(value, hras_sequence, aminoacids, start_position)\n",
    "    objects[key] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the objects are created and stored in a dictionary, we will use the method ``object.heatmap``. You will note that the first heatmap (\"population\") looks a bit washed out. If you look at the kernel distribution, the spread is smaller. The \"kernel\" and \"wt\" heatmaps look almost identical, while the \"counts\" heatmap looks all blue. This is caused by the algorithm not being able to center the data properly, and everything seems to be loss of function. That is why it is important to select the method of normalization that works with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T17:39:51.095369Z",
     "start_time": "2020-10-01T17:39:02.718714Z"
    }
   },
   "outputs": [],
   "source": [
    "titles: List[str] = ['population', 'counts', 'wt', 'kernel']\n",
    "\n",
    "# Create objects\n",
    "for obj, title in zip(objects.values(), titles):\n",
    "    obj.heatmap(title='Normalization by ' + title + ' method', output_file=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-07T18:16:01.195070Z",
     "start_time": "2020-09-07T18:16:01.189483Z"
    }
   },
   "source": [
    ".. image:: images/exported_images/hras_heatmap_norm_population.png\n",
    "\n",
    ".. image:: images/exported_images/hras_heatmap_norm_counts.png\n",
    "   \n",
    ".. image:: images/exported_images/hras_heatmap_norm_wt.png\n",
    "   \n",
    ".. image:: images/exported_images/hras_heatmap_norm_kernel.png\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('mutagenesis-visualization-NsjifA68-py3.9': venv)",
   "name": "python392jvsc74a57bd0722ba8cd244824779673bda02dd3c9429b019b25bafc8b2e589590aff5be60cf"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215.025px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}