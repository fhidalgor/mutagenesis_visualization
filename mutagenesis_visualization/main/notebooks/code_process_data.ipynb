{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T03:14:26.461366Z",
     "start_time": "2020-10-19T03:14:26.076297Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from collections import OrderedDict\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from scipy import stats\n",
    "from logomaker import alignment_to_matrix\n",
    "\n",
    "try:\n",
    "    import mutagenesis_visualization.main.scripts.code_utils as code_utils\n",
    "except ModuleNotFoundError:\n",
    "    import import_notebook\n",
    "    import os\n",
    "    directory = os.getcwd()\n",
    "    new_directory = directory.replace('tests', 'main')\n",
    "    os.chdir(new_directory)\n",
    "    import code_utils as code_utils\n",
    "    os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process trimmed fastq file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T03:14:26.514544Z",
     "start_time": "2020-10-19T03:14:26.464539Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_reads(\n",
    "    dna_sequence,\n",
    "    input_file: Union[str, Path],\n",
    "    codon_list='NNS',\n",
    "    counts_wt=True,\n",
    "    start_position=2,\n",
    "    output_file: Union[None, str, Path] = None,\n",
    "    full=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a trimmed fastq file containing DNA reads and returns the counts of\n",
    "    each DNA sequence specified by the user.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    dna_sequence : str,\n",
    "        Contains the DNA sequence of the allele of reference (usually wild-type).\n",
    "\n",
    "    input_file : str, default None\n",
    "        Path and name of the fastq file (full name including suffix \".fastq\").\n",
    "\n",
    "    codon_list : list or str, default 'NNS'\n",
    "        Input a list of the codons that were used to create point mutations. \n",
    "        Example: [\"GCC\", \"GCG\", \"TGC\"].\n",
    "        If the library was built using NNS and NNK codons, it is enough to input \n",
    "        'NNS' or 'NNK' as a string. It is important to know that the order of \n",
    "        the codon_list will determine the output order.\n",
    "\n",
    "    counts_wt : boolean, default True\n",
    "        If true it will add the counts to the wt allele. If false, it will set it up to np.nan.\n",
    "\n",
    "    start_position : int, default 2\n",
    "        First position in the protein sequence that will be used for the first column of the\n",
    "        array. If a protein has been mutated only from residue 100-150, then if start_position = 100,\n",
    "        the algorithm will trim the first 99 amino acids in the input sequence. The last\n",
    "        residue will be calculated based on the length of the input array. We have set the default value to 2\n",
    "        because normally the Methionine in position 1 is not mutated.\n",
    "\n",
    "    output_file : str, default None\n",
    "        If you want to export the generated files, add the path and name of the file without suffix.\n",
    "        Example: 'path/filename.xlsx'.\n",
    "\n",
    "    full: bool, optional\n",
    "        Switch determining nature of return value.\n",
    "        When it is False (the default) just the reads are\n",
    "        returned, when True diagnostic information from the\n",
    "        fastq analysis is also returned.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    df_counts : dataframe\n",
    "        Dataframe with the counts for each point mutant.\n",
    "\n",
    "    wt_counts : list\n",
    "        List of the counts for each for each DNA sequence that codes for the wild-type protein.\n",
    "\n",
    "    useful_reads : str\n",
    "        Present only if `full` = True. Contains the useful reads.\n",
    "\n",
    "    \"\"\"\n",
    "    # Assert messages\n",
    "    assert len(\n",
    "        dna_sequence\n",
    "    ) % 3 == 0, 'The dna_sequence length is not a multiple of 3'\n",
    "\n",
    "    # Make upper case in case input was lower case\n",
    "    dna_sequence = dna_sequence.upper()\n",
    "    if isinstance(codon_list, str):  #check if codon_list is a String\n",
    "        codon_list = codon_list.upper()\n",
    "    else:\n",
    "        codon_list = [item.upper() for item in codon_list]\n",
    "\n",
    "    # Create list with codons of sequence\n",
    "    wtSeqList = [dna_sequence[i:i + 3] for i in range(0, len(dna_sequence), 3)]\n",
    "\n",
    "    # codon_list\n",
    "    if codon_list == 'NNS':\n",
    "        codon_list = [\n",
    "            \"GCC\", \"GCG\", \"TGC\", \"GAC\", \"GAG\", \"TTC\", \"GGC\", \"GGG\", \"CAC\",\n",
    "            \"ATC\", \"AAG\", \"CTC\", \"CTG\", \"TTG\", \"ATG\", \"AAC\", \"CCC\", \"CCG\",\n",
    "            \"CAG\", \"CGC\", \"CGG\", \"AGG\", \"TCC\", \"TCG\", \"AGC\", \"ACC\", \"ACG\",\n",
    "            \"GTC\", \"GTG\", \"TGG\", \"TAC\", \"TAG\"\n",
    "        ]\n",
    "    elif codon_list == 'NNK':\n",
    "        codon_list = [\n",
    "            'GCG', 'GCT', 'TGT', 'GAT', 'GAG', 'TTT', 'GGG', 'GGT', 'CAT',\n",
    "            'ATT', 'AAG', 'CTG', 'CTT', 'TTG', 'ATG', 'AAT', 'CCG', 'CCT',\n",
    "            'CAG', 'AGG', 'CGG', 'CGT', 'AGT', 'TCG', 'TCT', 'ACG', 'ACT',\n",
    "            'GTG', 'GTT', 'TGG', 'TAT', 'TAG'\n",
    "        ]\n",
    "\n",
    "    # Enumerate variants\n",
    "    variants = _enumerate_variants(wtSeqList, codon_list, dna_sequence)\n",
    "\n",
    "    # Count variant frequency\n",
    "    variants, totalreads, usefulreads = count_fastq(variants, input_file)\n",
    "\n",
    "    # Convert to df\n",
    "    wtProtein = Seq(dna_sequence).translate()\n",
    "    df = pd.DataFrame()\n",
    "    df['Position'] = np.ravel(\n",
    "        [[pos] * len(codon_list)\n",
    "         for pos in np.arange(start_position,\n",
    "                              len(wtProtein) + start_position).astype(int)]\n",
    "    )\n",
    "    df['Codon'] = codon_list * len(wtProtein)\n",
    "    df['WTCodon'] = np.ravel([[codon] * len(codon_list) for codon in wtSeqList])\n",
    "    df['Aminoacid'] = np.ravel([[aa] * len(codon_list) for aa in wtProtein])\n",
    "    df['SynWT'] = df.apply(\n",
    "        lambda x: _are_syn(x['Codon'], x['WTCodon'], _codon_table()), axis=1\n",
    "    )\n",
    "    df['Counts'] = list(variants.values())\n",
    "\n",
    "    if counts_wt:\n",
    "        try:  # try is to fix the Bug Che discovered\n",
    "            df.loc[df['Codon'] == df['WTCodon'],\n",
    "                   'Counts'] = variants[dna_sequence]\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        df.loc[df['Codon'] == df['WTCodon'], 'Counts'] = np.nan\n",
    "\n",
    "    # Pivot table and reindex\n",
    "    df_counts = df.pivot_table(\n",
    "        values='Counts', index='Codon', columns=['Position'], dropna=False\n",
    "    )\n",
    "    df_counts = df_counts.reindex(index=codon_list)\n",
    "\n",
    "    # Get WT counts syn. Added or operator so also chooses WT codon\n",
    "    df_wt = df.loc[(df['SynWT'] == True) | (df['SynWT'] == 'wt codon')][[ # perhaps I need to remove this again\n",
    "        'Counts' # removed\n",
    "    ]]\n",
    "\n",
    "    # Export files\n",
    "    if output_file:\n",
    "        # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "        writer = pd.ExcelWriter(str(output_file), engine='xlsxwriter')\n",
    "\n",
    "        # export\n",
    "        df_counts.to_excel(writer, sheet_name='Counts', index=True)\n",
    "        df_wt.to_excel(writer, sheet_name='WT', index=False)\n",
    "\n",
    "        # Close the Pandas Excel writer and output the Excel file.\n",
    "        writer.save()\n",
    "\n",
    "    if full:\n",
    "        # Print total reads\n",
    "        percent_useful = usefulreads / totalreads * 100\n",
    "        return df_counts, df_wt, f\"{usefulreads}/{totalreads} useful reads ({percent_useful:.1f}%)\"\n",
    "    else:\n",
    "        return df_counts, df_wt\n",
    "\n",
    "\n",
    "def _codon_table():\n",
    "    codontable = {\n",
    "        'ATA': 'I', 'ATC': 'I', 'ATT': 'I', 'ATG': 'M', 'ACA': 'T', 'ACC': 'T',\n",
    "        'ACG': 'T', 'ACT': 'T', 'AAC': 'N', 'AAT': 'N', 'AAA': 'K', 'AAG': 'K',\n",
    "        'AGC': 'S', 'AGT': 'S', 'AGA': 'R', 'AGG': 'R', 'CTA': 'L', 'CTC': 'L',\n",
    "        'CTG': 'L', 'CTT': 'L', 'CCA': 'P', 'CCC': 'P', 'CCG': 'P', 'CCT': 'P',\n",
    "        'CAC': 'H', 'CAT': 'H', 'CAA': 'Q', 'CAG': 'Q', 'CGA': 'R', 'CGC': 'R',\n",
    "        'CGG': 'R', 'CGT': 'R', 'GTA': 'V', 'GTC': 'V', 'GTG': 'V', 'GTT': 'V',\n",
    "        'GCA': 'A', 'GCC': 'A', 'GCG': 'A', 'GCT': 'A', 'GAC': 'D', 'GAT': 'D',\n",
    "        'GAA': 'E', 'GAG': 'E', 'GGA': 'G', 'GGC': 'G', 'GGG': 'G', 'GGT': 'G',\n",
    "        'TCA': 'S', 'TCC': 'S', 'TCG': 'S', 'TCT': 'S', 'TTC': 'F', 'TTT': 'F',\n",
    "        'TTA': 'L', 'TTG': 'L', 'TAC': 'Y', 'TAT': 'Y', 'TAA': '*', 'TAG': '*',\n",
    "        'TGC': 'C', 'TGT': 'C', 'TGA': '*', 'TGG': 'W'\n",
    "    }\n",
    "    return codontable\n",
    "\n",
    "\n",
    "def _are_syn(codon1, codon2, codontable):\n",
    "    """Determine if 2 codons are synonymous"""\n",
    "    if codon1 == codon2:\n",
    "        return 'wt codon' # changed from False\n",
    "    if _translate(codon1, codontable) is not _translate(codon2, codontable):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _translate(seq, codontable):\n",
    "    """Translate DNA sequence to protein."""\n",
    "    # I forgot why I made this custom function instead of using a biopython function\n",
    "    protein = ''\n",
    "    if len(seq) % 3 == 0:\n",
    "        for i in range(0, len(seq), 3):\n",
    "            codon = seq[i:i + 3]\n",
    "            protein += codontable[codon]\n",
    "    return protein\n",
    "\n",
    "\n",
    "def _enumerate_variants(wtSeqList, codon_list, dna_sequence):\n",
    "    """Will return an ordered dictionary with variants initialized to 0 counts"""\n",
    "    # Create ordered dictionary\n",
    "    variants = OrderedDict()\n",
    "\n",
    "    # First instance that we see the wt?\n",
    "    firstwtseq = False\n",
    "\n",
    "    # Loop over codons\n",
    "    for position in range(0, len(wtSeqList)):\n",
    "        for codons in (codon_list):\n",
    "            variant = ''.join(wtSeqList[0:position]) + \\\n",
    "                codons + ''.join(wtSeqList[position+1:])\n",
    "            if (variant == dna_sequence):  # Store redundant wild-types\n",
    "                if firstwtseq:\n",
    "                    variant = 'wtSeq' + str(position)\n",
    "                firstwtseq = True\n",
    "            variants[variant] = 0\n",
    "    return variants\n",
    "\n",
    "\n",
    "def count_fastq(variants, input_file):\n",
    "    """\n",
    "    Count the frequency of variants in the input fastq file. \n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    variants : ordered dict\n",
    "        Contains each DNA sequence that you want to count from the fastq file.\n",
    "        If your input is a list of strings, use the auxiliar function _initialize_ordereddict\n",
    "        to convert it to an ordered dictionary.\n",
    "        If you input a list, it will convert it to an ordered dict.\n",
    "\n",
    "    input_file : str, default None\n",
    "        Path and name of the fastq file (full name including suffix \".fastq\").\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    variants : ordered dict\n",
    "        Same input dictionary by now has the values updated with the counts.\n",
    "    totalreads : int\n",
    "        Total number of DNA chains that appear in the fastq file.\n",
    "    usefulreads : int\n",
    "        Total number of identified DNA chains. Calculated as the sum of all the key values.\n",
    "    """\n",
    "    # if variant input is not an ordered dict, convert to ordered dict\n",
    "    if not (isinstance(variants, OrderedDict)):\n",
    "        variants = _initialize_ordereddict(variants)\n",
    "\n",
    "    # iterate over fastq file and count reads\n",
    "    totalreads = 0\n",
    "    for nuc in SeqIO.parse(str(input_file), \"fastq\"):\n",
    "        totalreads += 1\n",
    "        nucleicsequence = str(nuc.seq)\n",
    "        if nucleicsequence in variants:\n",
    "            variants[nucleicsequence] += 1\n",
    "    usefulreads = np.nansum(list(variants.values()))\n",
    "    return variants, totalreads, usefulreads\n",
    "\n",
    "\n",
    "def _initialize_ordereddict(list_variants):\n",
    "    """\n",
    "    Will return an ordered dictionary with variants initialized to 0 counts.\n",
    "    Here the user specifies the variants as a list.\n",
    "\n",
    "    This function should be used when you want to use _count_fastq\n",
    "    \n",
    "    """\n",
    "\n",
    "    # Create normal dictionary\n",
    "    dictionary = dict(zip(list_variants, np.zeros(len(list_variants))))\n",
    "\n",
    "    # Create ordered dictionary\n",
    "    variants = OrderedDict(dictionary)\n",
    "\n",
    "    return variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process count files and return enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T03:14:26.533323Z",
     "start_time": "2020-10-19T03:14:26.516835Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_enrichment(\n",
    "    pre_lib,\n",
    "    post_lib,\n",
    "    pre_wt=None,\n",
    "    post_wt=None,\n",
    "    aminoacids=list('AACDEFGGHIKLLLMNPPQRRRSSSTTVVWY*'),\n",
    "    zeroing='population',\n",
    "    how='median',\n",
    "    norm_std=True,\n",
    "    stopcodon=False,\n",
    "    min_counts=25,\n",
    "    min_countswt=100,\n",
    "    std_scale=0.2,\n",
    "    mpop=2,\n",
    "    mad_filtering=True,\n",
    "    mwt=2,\n",
    "    infinite=3,\n",
    "    output_file: Union[None, str, Path] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Determine the enrichment scores of a selection experiment, where there is a\n",
    "    preselected population (input) and a selected population (output).\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    pre_lib : str, pandas dataframe or np.array\n",
    "        Can be filepath and name of the exported txt file, dataframe or np.array.\n",
    "\n",
    "    post_lib : str, pandas dataframe or np.array\n",
    "        Can be filepath and name of the exported txt file, dataframe or np.array.\n",
    "\n",
    "    pre_wt : str, or np.array, optional\n",
    "        Str with filepath and name of the exported txt file or np.array.\n",
    "\n",
    "    post_wt : str, or np.array, optional\n",
    "        Str with filepath and name of the exported txt file or np.array.\n",
    "\n",
    "    aminoacids : list, default ('AACDEFGGHIKLLLMNPPQRRRSSSTTVVWY*')\n",
    "        Index of aminoacids (in order). Stop codon needs to be '*'.\n",
    "\n",
    "    zeroing : str, default 'population'\n",
    "        Method to normalize the data.\n",
    "        Can also use 'none', 'zscore', 'counts', 'wt' or 'kernel'. If 'wt' is used 'pre_wt' must not be set to None.\n",
    "\n",
    "    how : str, default 'median'\n",
    "        Metric to zero the data. Only works if zeroing='population' or 'wt'.\n",
    "        Can also be set to 'mean' or 'mode'.\n",
    "\n",
    "    norm_std : boolean, default True\n",
    "        If norm_std is set to True, it will scale the data.\n",
    "\n",
    "    stopcodon : boolean, default False\n",
    "        Use the enrichment score stop codons as a metric to determine the minimum enrichment score.\n",
    "\n",
    "    min_counts : int, default 25\n",
    "        If mutant has less than the min_counts, it will be replaced by np.nan.\n",
    "\n",
    "    min_countswt : int, default 100\n",
    "        If synonymous wild-type mutant has less than the min_counts, it will be replaced by np.nan.\n",
    "\n",
    "    std_scale : float, default 0.2\n",
    "        Factor by which the population is scaled. Only works if norm_std is set to True.\n",
    "\n",
    "    mpop : int, default 2\n",
    "        When using the median absolute deviation (MAD) filtering, mpop is the number of medians away\n",
    "        a data point must be to be discarded.\n",
    "    \n",
    "    mad_filtering : boolean, default True\n",
    "        Will apply MAD filtering to data.\n",
    "        \n",
    "    mwt : int, default 2\n",
    "        When MAD filtering, mpop is the number of medians away a data point must be to\n",
    "        be discarded. The difference with mpop is that mwt is only used when the population of wild-type\n",
    "        alleles is the reference for data zeroing.\n",
    "\n",
    "    infinite : int, default 3\n",
    "        It will replace +infinite values with +3 and -infinite with -3.\n",
    "\n",
    "    output_file : str, default None\n",
    "        If you want to export the generated files, add the path and name of the file without suffix.\n",
    "        Example: 'path/filename'. File will be save as a txt file.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    zeroed : ndarray\n",
    "        A np.array containing the enrichment scores.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy if libraries are in dataframe format.\n",
    "    # If input is a filepath, then load the txt files\n",
    "    if type(pre_lib) is pd.DataFrame:\n",
    "        pre_lib = pre_lib.to_numpy()\n",
    "    elif type(pre_lib) is str:\n",
    "        pre_lib = np.loadtxt(pre_lib)\n",
    "    if type(post_lib) is pd.DataFrame:\n",
    "        post_lib = post_lib.to_numpy()\n",
    "    elif type(post_lib) is str:\n",
    "        post_lib = np.loadtxt(post_lib)\n",
    "\n",
    "    # Same thing for wt allele files\n",
    "    if type(pre_wt) is str:\n",
    "        pre_wt = np.loadtxt(pre_wt)\n",
    "    if type(post_wt) is str:\n",
    "        post_wt = np.loadtxt(post_wt)\n",
    "\n",
    "    # Convert to df\n",
    "    pre_lib = _array_to_df_enrichments(pre_lib, aminoacids)\n",
    "    post_lib = _array_to_df_enrichments(post_lib, aminoacids)\n",
    "\n",
    "    # Locate stop codons\n",
    "    if stopcodon:\n",
    "        input_stopcodon = pre_lib.loc['*'].astype(float)\n",
    "        output_stopcodon = post_lib.loc['*'].astype(float)\n",
    "    else:\n",
    "        input_stopcodon = ''\n",
    "        output_stopcodon = ''\n",
    "\n",
    "    # Log10 of the counts for library and wt alleles\n",
    "    log10_counts = _get_enrichment(\n",
    "        pre_lib, post_lib, input_stopcodon, output_stopcodon, min_counts,\n",
    "        stopcodon, infinite\n",
    "    )\n",
    "    # Group by amino acid\n",
    "    df = pd.DataFrame(data=log10_counts)\n",
    "    log10_counts_grouped = _group_byaa(df, aminoacids)\n",
    "\n",
    "    # MAD filtering\n",
    "    if mad_filtering:\n",
    "        log10_counts_mad = _MAD_filtering(\n",
    "            np.ravel(np.array(log10_counts_grouped)), mpop\n",
    "        )\n",
    "    else:\n",
    "        log10_counts_mad = np.ravel(np.array(log10_counts_grouped))\n",
    "    \n",
    "    # Statistics population using mad filtered data\n",
    "    mean_pop = np.nanmean(log10_counts_mad)\n",
    "    median_pop = np.nanmedian(log10_counts_mad)\n",
    "    std_pop = np.nanstd(log10_counts_mad)\n",
    "    mode_pop = _nanmode(log10_counts_mad)\n",
    "\n",
    "    # Wt counts\n",
    "    if pre_wt is not None:\n",
    "        log10_wtcounts = _get_enrichment(\n",
    "            pre_wt, post_wt, input_stopcodon, output_stopcodon, min_countswt,\n",
    "            stopcodon, infinite\n",
    "        )\n",
    "        # MAD filtering\n",
    "        # If set to m=1, if tosses out about 50% of the values. the mean barely changes though\n",
    "        if mad_filtering:\n",
    "            log10_wtcounts = _MAD_filtering(log10_wtcounts, mwt)\n",
    "            \n",
    "        mean_wt = np.nanmean(log10_wtcounts)\n",
    "        median_wt = np.nanmedian(log10_wtcounts)\n",
    "        std_wt = np.nanstd(log10_wtcounts)\n",
    "        if len(log10_wtcounts)>1:\n",
    "            mode_wt = _nanmode(log10_wtcounts)\n",
    "        else: #case for only 1 wt\n",
    "            mode_wt = log10_wtcounts\n",
    "            \n",
    "    # Zero data, select case\n",
    "    if zeroing == 'wt':\n",
    "        if how == 'mean':\n",
    "            zeroed = log10_counts_grouped - mean_wt\n",
    "        elif how == 'median':\n",
    "            zeroed = log10_counts_grouped - median_wt\n",
    "        elif how == 'mode':\n",
    "            zeroed = log10_counts_grouped - mode_wt\n",
    "        elif norm_std == True:\n",
    "            zeroed = zeroed * std_scale / 2 / std_wt\n",
    "    elif zeroing == 'population':\n",
    "        if how == 'mean':\n",
    "            zeroed = log10_counts_grouped - mean_pop\n",
    "        elif how == 'median':\n",
    "            zeroed = log10_counts_grouped - median_pop\n",
    "        elif how == 'mode':\n",
    "            zeroed = log10_counts_grouped - mode_pop\n",
    "        elif norm_std == True:\n",
    "            zeroed = zeroed * std_scale / std_pop\n",
    "    elif zeroing == 'counts':\n",
    "        # Get the ratio of counts\n",
    "        ratio = np.log10(post_lib.sum().sum()/pre_lib.sum().sum())\n",
    "        zeroed = log10_counts_grouped + ratio\n",
    "        if norm_std == True:\n",
    "            zeroed = zeroed * std_scale / std_pop\n",
    "    elif zeroing == 'kernel':\n",
    "        zeroed_0, kernel_std = _kernel_correction(\n",
    "            log10_counts_grouped, aminoacids\n",
    "        )\n",
    "        zeroed, kernel_std = _kernel_correction(zeroed_0, aminoacids, cutoff=1)\n",
    "        if norm_std is True:\n",
    "            zeroed = zeroed * std_scale / kernel_std\n",
    "    elif zeroing == 'zscore':\n",
    "        zeroed = stats.zscore(log10_counts_grouped, nan_policy='omit')\n",
    "    elif zeroing == 'none':\n",
    "        zeroed = log10_counts_grouped\n",
    "    else:\n",
    "        raise ValueError('Wrong zeroed parameter')\n",
    "    # Export files\n",
    "    if output_file:\n",
    "        np.savetxt(Path(output_file), zeroed, fmt='%i', delimiter='\\t')\n",
    "\n",
    "    return zeroed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T03:14:26.556669Z",
     "start_time": "2020-10-19T03:14:26.535208Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_enrichment(\n",
    "    input_lib, output_lib, input_stopcodon, output_stopcodon, min_counts,\n",
    "    stopcodon, infinite\n",
    "):\n",
    "    """Calculate log10 enrichment scores from input and output counts"""\n",
    "    # Copy data and replace low counts by np.nan\n",
    "    input_lib = np.copy(input_lib.astype(float))\n",
    "    output_lib = np.copy(output_lib.astype(float))\n",
    "    input_lib[input_lib < min_counts] = np.nan\n",
    "\n",
    "    # Stop codon correction\n",
    "    if stopcodon:\n",
    "        output_lib = _stopcodon_correction(\n",
    "            input_lib, output_lib, input_stopcodon, output_stopcodon\n",
    "        )\n",
    "\n",
    "    # log10 of library and replace infinite values. This will potentially divide by zero.\n",
    "    with np.errstate(divide='ignore'):\n",
    "        counts_log10_ratio = _replace_inf(\n",
    "            np.log10(output_lib / input_lib), infinite\n",
    "        )\n",
    "\n",
    "    return counts_log10_ratio\n",
    "\n",
    "\n",
    "def _stopcodon_correction(\n",
    "    input_lib, output_lib, input_stopcodon, output_stopcodon\n",
    "):\n",
    "    """This aux function will take as an input the counts for pre and post selection (and also for wT subset), \n",
    "    and will return the corrected output counts"""\n",
    "\n",
    "    # calculate stop codons frequencies\n",
    "    frequency_stopcodons = output_stopcodon / input_stopcodon\n",
    "\n",
    "    # MAD filtering\n",
    "    frequency_stopcodons_filtered = _MAD_filtering(frequency_stopcodons, m=2)\n",
    "    median_frequency = np.nanmedian(frequency_stopcodons_filtered)\n",
    "\n",
    "    # subtract to output counts\n",
    "    output_lib_corr = output_lib - input_lib * median_frequency\n",
    "\n",
    "    # eliminate negative values so they wont get turned into np.nan\n",
    "    output_lib_corr[output_lib_corr < 0] = 0\n",
    "\n",
    "    return output_lib_corr\n",
    "\n",
    "\n",
    "def _MAD_filtering(data, m=2):\n",
    "    """This aux function will take a numpy array, calculate median and MAD, \n",
    "    and filter the data removing outliers"""\n",
    "\n",
    "    # turn data into df to do mad calculations\n",
    "    df = pd.DataFrame(np.array(data), columns=['Data'])\n",
    "    median = df['Data'].median(axis=0)\n",
    "    mad = df['Data'].mad(axis=0)\n",
    "    df['Abs_Dev'] = np.abs(data - median) / mad\n",
    "\n",
    "    # filter values m times away from median, by default m = 2\n",
    "    df['Abs_Dev'].mask(df['Abs_Dev'] > m, inplace=True)  # mask values\n",
    "    df.dropna(how='any', inplace=True)  # eliminte NANs\n",
    "\n",
    "    return df['Data'].to_numpy()\n",
    "\n",
    "\n",
    "def _replace_inf(array, infinite):\n",
    "    """Replace values over a threshold with a min or max value"""\n",
    "    np.warnings.filterwarnings('ignore')\n",
    "    array[array == -np.inf] = -infinite\n",
    "    array[array < -infinite] = -infinite\n",
    "    array[array == +np.inf] = +infinite\n",
    "    array[array > +infinite] = +infinite\n",
    "    return array\n",
    "\n",
    "\n",
    "def _group_byaa(df, aminoacids):\n",
    "    """Group different codons that are synonymous"""\n",
    "    # copy df\n",
    "    df = df.copy()\n",
    "\n",
    "    # Set up amino acid column\n",
    "    df['Aminoacid'] = aminoacids\n",
    "\n",
    "    # Group by mean\n",
    "    df = df.groupby(as_index=True, by='Aminoacid', sort=False).mean()\n",
    "    return df\n",
    "\n",
    "\n",
    "def _nanmode(data):\n",
    "    """\n",
    "    Input is wt log enrichments, and return the mode of the histogram \n",
    "    (aka the x coordinate at which y is max).\n",
    "    \n",
    "    """\n",
    "\n",
    "    # Copy data\n",
    "    data = np.copy(data)\n",
    "    # Remove NaN values\n",
    "    data_corrected = data[np.invert(np.isnan(data))]\n",
    "    # Adjust kernel\n",
    "    kernel_processed_data = stats.gaussian_kde(data_corrected)\n",
    "    # Find mode\n",
    "    indexmax = np.where(\n",
    "        kernel_processed_data(data_corrected) ==\n",
    "        kernel_processed_data(data_corrected).max()\n",
    "    )\n",
    "    # Return mean in case there are two x values with equal y-axis height\n",
    "    return data_corrected[indexmax].mean()\n",
    "\n",
    "\n",
    "# corrects the mutagenesis data and returns the height of the peak\n",
    "def _kernel_correction(data, aminoacids, cutoff=2):\n",
    "    """input the library matrix, returns the corrected version. I set to 0 the max of the peak of the normal dist\n",
    "    ignores stop codons. Not used for dataframes, only numpy arrays"""\n",
    "\n",
    "    # Get data into right format\n",
    "    data_corrected, kernel_processed_data = _kernel_datapreparation(\n",
    "        data, cutoff\n",
    "    )\n",
    "\n",
    "    # Find max of kernel peak\n",
    "    indexmax = np.where(\n",
    "        kernel_processed_data(data_corrected) ==\n",
    "        kernel_processed_data(data_corrected).max()\n",
    "    )\n",
    "\n",
    "    # Normalize the max of peak os it has an x = 0\n",
    "    data_final = data - data_corrected[indexmax].mean()\n",
    "\n",
    "    # find std of kernel. It uses the already max peak x=0 normalized data\n",
    "    data_final_flatten, data_final_kernel_processed_data = _kernel_datapreparation(\n",
    "        data_final, cutoff\n",
    "    )\n",
    "    std = _kernel_std(data_final_flatten, data_final_kernel_processed_data)\n",
    "\n",
    "    return data_final, std\n",
    "\n",
    "\n",
    "def _kernel_datapreparation(data, cutoff):\n",
    "    """\n",
    "    This function will copy the data, eliminate stop codon, eliminate values lower than -1, \n",
    "    flatten and eliminate np.nan. Will return the data in that format + the adjusted kernel\n",
    "    \n",
    "    """\n",
    "    \n",
    "    # Eliminate stop codon\n",
    "    data_corrected = np.array(data.drop('*', errors='ignore').copy())\n",
    "\n",
    "    # Eliminate values lower than -1\n",
    "    data_corrected = data_corrected[(data_corrected >= -cutoff)\n",
    "                                    & (data_corrected <= cutoff)]\n",
    "\n",
    "    # Get rid of np.nan values and convert matrix into 1d matrix\n",
    "    data_corrected = data_corrected[np.invert(np.isnan(data_corrected))]\n",
    "\n",
    "    # Adjust gaussian kernel\n",
    "    kernel_processed_data = stats.gaussian_kde(data_corrected)\n",
    "\n",
    "    return data_corrected, kernel_processed_data\n",
    "\n",
    "\n",
    "def _kernel_std(data, kernel):\n",
    "    """\n",
    "    Input the library matrix (and wont count stop codon), and will return the std of the normal distribution.\n",
    "    To calculate the std, it will find the FWHM and divide by 2.355\n",
    "    https://en.wikipedia.org/wiki/Full_width_at_half_maximum\n",
    "    The algorithm will give back the min std between both sides of the peak.\n",
    "    \n",
    "    """\n",
    "\n",
    "    # find ymax and the x value of the max height\n",
    "    y_max = kernel(data).max()\n",
    "    index_y_max = np.where(kernel(data) == y_max)\n",
    "    x_ymax = data[index_y_max].mean()\n",
    "\n",
    "    # find the two x value of ymax/2. One at each side of the center. l of left and r of right\n",
    "    # so I can select only the positive side of the distribution\n",
    "    y_temp = kernel(data)\n",
    "\n",
    "    # left side\n",
    "    y_hw_l = (min(y_temp[data < 0], key=lambda x: abs(x - y_max / 2)))\n",
    "    index_yhw_l = np.where(kernel(data) == y_hw_l)\n",
    "    x_yhw_l = data[index_yhw_l].mean()\n",
    "\n",
    "    # right side\n",
    "    y_hw_r = (min(y_temp[data > 0], key=lambda x: abs(x - y_max / 2)))\n",
    "    index_yhw_r = np.where(kernel(data) == y_hw_r)\n",
    "    x_yhw_r = data[index_yhw_r].mean()\n",
    "\n",
    "    # calculate half width at half maximum\n",
    "    hwhm_l = abs(x_yhw_l - x_ymax)\n",
    "    hwhm_r = abs(x_yhw_r - x_ymax)\n",
    "\n",
    "    # calculate std from fwhm\n",
    "    std_l = hwhm_l / ((2 * np.log(2))**0.5)\n",
    "    std_r = hwhm_r / ((2 * np.log(2))**0.5)\n",
    "\n",
    "    return min(std_l, std_r)\n",
    "\n",
    "\n",
    "def _array_to_df_enrichments(lib, aminoacids):\n",
    "    """\n",
    "    aux function to transform array in df with index of amino acids.\n",
    "    \n",
    "    """\n",
    "    \n",
    "    df = pd.DataFrame(index=aminoacids, data=lib)\n",
    "    return df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble sublibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T03:14:26.575348Z",
     "start_time": "2020-10-19T03:14:26.559476Z"
    }
   },
   "outputs": [],
   "source": [
    "def assemble_sublibraries(\n",
    "    excel_path,\n",
    "    sheet_pre,\n",
    "    sheet_post,\n",
    "    columns,\n",
    "    nrows_pop,\n",
    "    nrows_wt,\n",
    "    columns_wt=None,\n",
    "    skiprows=1,\n",
    "    aminoacids=list('AACDEFGGHIKLLLMNPPQRRRSSSTTVVWY*'),\n",
    "    zeroing='population',\n",
    "    how='median',\n",
    "    norm_std=True,\n",
    "    stopcodon=False,\n",
    "    min_counts=25,\n",
    "    min_countswt=100,\n",
    "    std_scale=0.2,\n",
    "    mpop=2,\n",
    "    mwt=2,\n",
    "    infinite=3,\n",
    "    output_file: Union[None, str, Path] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Assembles different sublibraries into one. Uses calculate_enrichments. Can\n",
    "    only read from excel files that are in the same format as the example\n",
    "    provided.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    excel_path : str\n",
    "        Location of the excel file to read.\n",
    "\n",
    "    sheet_pre : str\n",
    "        Name of the sheet with input (pre-selected) counts.\n",
    "\n",
    "    sheet_post : str\n",
    "        Name of the sheet with output (post-selected) counts.\n",
    "\n",
    "    columns : list\n",
    "        List of columns for each sublibrary to read from the excel file.\n",
    "\n",
    "    nrows_pop : int,\n",
    "        Number of rows to read from the excel.\n",
    "\n",
    "    nrows_wt : list,\n",
    "        Contains a list of integers, with the number of rows to read from each wt subset.\n",
    "\n",
    "    columns_wt : list,\n",
    "        Contains a list of strings, specifying the excel columns to read for each wt subset.\n",
    "\n",
    "    skiprows : int, default 1\n",
    "        Parameter for pd.read_excel. Only works for the main columns, not for wt.\n",
    "\n",
    "    aminoacids : list, default ('AACDEFGGHIKLLLMNPPQRRRSSSTTVVWY*')\n",
    "        Index of aminoacids (in order). Stop codon needs to be '*'.\n",
    "\n",
    "    zeroing : str, default 'population'\n",
    "        Method to zero the data.\n",
    "        Can also use 'counts', wt' or 'kernel'.\n",
    "\n",
    "    how : str, default 'median'\n",
    "        Metric to zero the data. Only works if zeroing='population' or 'wt'.\n",
    "        Can also be set to 'mean' or 'mode'.\n",
    "\n",
    "    norm_std : boolean, default True\n",
    "        If norm_std is set to True, it will scale the data.\n",
    "\n",
    "    stopcodon : boolean, default False\n",
    "        Use the enrichment score stop codons as a metric to determine the minimum enrichment score.\n",
    "\n",
    "    min_counts : int, default 25\n",
    "        If mutant has less than the min_counts, it will be replaced by np.nan.\n",
    "\n",
    "    min_countswt : int, default 100\n",
    "        If synonymous wild-type mutant has less than the min_counts, it will be replaced by np.nan.\n",
    "\n",
    "    std_scale : float, default 0.2\n",
    "        Factor by which the population is scaled. Only works if norm_std is set to True.\n",
    "\n",
    "    mpop : int, default 2\n",
    "        When using the median absolute deviation (MAD) filtering, mpop is the number of medians away\n",
    "        a data point must be to be discarded.\n",
    "\n",
    "    mwt : int, default 2\n",
    "        When MAD filtering, mpop is the number of medians away a data point must be to\n",
    "        be discarded. The difference with mpop is that mwt is only used when the population of wild-type\n",
    "        alleles is the reference for data zeroing.\n",
    "\n",
    "    infinite : int, default 3\n",
    "        It will replace +infinite values with +3 and -infinite with -3.\n",
    "\n",
    "    output_file : str, default None\n",
    "        If you want to export the generated files, add the path and name of the file without suffix.\n",
    "        Example: 'path/filename'. File will be save as a txt file.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    df : Pandas dataframe\n",
    "        A dataframe that contains the enrichment scores of the assembled sublibraries.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Read reads from excel\n",
    "    list_pre, list_sel, list_pre_wt, list_sel_wt = _read_counts(\n",
    "        excel_path, sheet_pre, sheet_post, columns, nrows_pop, nrows_wt,\n",
    "        columns_wt\n",
    "    )\n",
    "\n",
    "    # Assemble sublibraries\n",
    "    df = _assemble_list(\n",
    "        list_pre, list_sel, list_pre_wt, list_sel_wt, aminoacids, zeroing, how,\n",
    "        norm_std, stopcodon, min_counts, min_countswt, std_scale, mpop, mwt,\n",
    "        infinite\n",
    "    )\n",
    "\n",
    "    # Export files\n",
    "    if output_file:\n",
    "        np.savetxt(Path(output_file), zeroed, fmt='%i', delimiter='\\t')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _read_counts(\n",
    "    excel_path,\n",
    "    sheet_pre,\n",
    "    sheet_post,\n",
    "    columns,\n",
    "    nrows_pop,\n",
    "    nrows_wt,\n",
    "    columns_wt=None,\n",
    "    skiprows=1\n",
    "):\n",
    "    """Aux"""\n",
    "    # Create dictionary with data. Loading 3 replicates, each of them is divided into 3 pools\n",
    "    list_pre, list_sel, list_pre_wt, list_sel_wt = ([] for i in range(4))\n",
    "\n",
    "    # Read counts from excel\n",
    "    replicates = np.arange(0, len(sheet_pre))\n",
    "    for column, column_wt, nrow_wt, rep in zip(columns, columns_wt, nrows_wt,\n",
    "                                               replicates):\n",
    "        # Pre counts\n",
    "        list_pre.append(\n",
    "            pd.read_excel(\n",
    "                excel_path,\n",
    "                sheet_pre,\n",
    "                skiprows=skiprows,\n",
    "                usecols=column,\n",
    "                nrows=nrows_pop\n",
    "            )\n",
    "        )\n",
    "        # Sel counts\n",
    "        list_sel.append(\n",
    "            pd.read_excel(\n",
    "                excel_path,\n",
    "                sheet_post,\n",
    "                skiprows=skiprows,\n",
    "                usecols=column,\n",
    "                nrows=nrows_pop\n",
    "            )\n",
    "        )\n",
    "        if columns_wt is None:\n",
    "            list_pre_wt.append(None)\n",
    "            list_sel_wt.append(None)\n",
    "        else:\n",
    "            # Pre counts wild-type alleles\n",
    "            list_pre_wt.append(\n",
    "                pd.read_excel(\n",
    "                    excel_path, sheet_pre, usecols=column_wt, nrows=nrow_wt\n",
    "                )\n",
    "            )\n",
    "            # Sel counts wild-type alleles\n",
    "            list_sel_wt.append(\n",
    "                pd.read_excel(\n",
    "                    excel_path, sheet_post, usecols=column_wt, nrows=nrow_wt\n",
    "                )\n",
    "            )\n",
    "    return list_pre, list_sel, list_pre_wt, list_sel_wt\n",
    "\n",
    "\n",
    "def _assemble_list(\n",
    "    list_pre,\n",
    "    list_sel,\n",
    "    list_pre_wt,\n",
    "    list_sel_wt,\n",
    "    aminoacids,\n",
    "    zeroing,\n",
    "    how,\n",
    "    norm_std,\n",
    "    stopcodon,\n",
    "    min_counts,\n",
    "    min_countswt,\n",
    "    std_scale,\n",
    "    mpop,\n",
    "    mwt,\n",
    "    infinite,\n",
    "    output_file: Union[None, str, Path] = None\n",
    "):\n",
    "    """\n",
    "    gets the output from _read_counts and assembles the sublibraries\n",
    "    \n",
    "    """\n",
    "\n",
    "    enrichment_lib = []\n",
    "\n",
    "    for pre, sel, pre_wt, sel_wt in zip(list_pre, list_sel, list_pre_wt,\n",
    "                                        list_sel_wt):\n",
    "        # log 10\n",
    "        enrichment_log10 = calculate_enrichment(\n",
    "            pre, sel, pre_wt, sel_wt, aminoacids, zeroing, how, norm_std,\n",
    "            stopcodon, min_counts, min_countswt, std_scale, mpop, mwt, infinite\n",
    "        )\n",
    "        # Store in list\n",
    "        enrichment_lib.append(enrichment_log10)\n",
    "\n",
    "    # Concatenate sublibraries\n",
    "    df = pd.concat(enrichment_lib, ignore_index=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge enrichment with MSA conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T03:14:26.592641Z",
     "start_time": "2020-10-19T03:14:26.578952Z"
    }
   },
   "outputs": [],
   "source": [
    "def msa_enrichment(self, path, start_position, threshold=0.01):\n",
    "    """\n",
    "    Generate a dataframe with the Shannon entropy by residue and the mean enrichment score, and\n",
    "    a second dataframe with the frequency of each substitution and the enrichment score\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    self : object from class *Screen*\n",
    "\n",
    "    path : str\n",
    "        Path where is located the fasta MSA that will be parsed. That MSA needs to have removed \n",
    "        any insertions that are not present in the target sequence. For example, if a Ras ortholog has \n",
    "        an extra amino acid at position 123, that needs to be removed from the aligment. Otherwise, everything\n",
    "        will be shifted by 1 residue.\n",
    "\n",
    "    start_position : int\n",
    "        This is the position in the protein sequence of the first position in the MSA.\n",
    "\n",
    "    threshold : float, default 0.01\n",
    "        The conservation frequency for each amino acid subsitution will be binarized, and a threshold between 0-1 \n",
    "        needs to be selected.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    df_shannon: pandas dataframe\n",
    "        Shannon entropy by residue and mean enrichment score by residue. \n",
    "\n",
    "    df_freq : pandas dataframe   \n",
    "        Frequency of each susbsitution merged to the enrichment score.\n",
    "    """\n",
    "    # Read MSA\n",
    "    msa, seq_lengths, index = code_utils._parseMSA(path, \"fasta\", 0)\n",
    "\n",
    "    # Calculate Shannon entropy from alignment\n",
    "    shannon_entropy = code_utils._shannon_entropy_list_msa(msa)\n",
    "\n",
    "    # Merge enrichment scores and MSA conservation\n",
    "    df_freq = _merge_msa_enrichment(\n",
    "        self, _msa_to_df(msa), start_position, threshold\n",
    "    )\n",
    "\n",
    "    # Merge shannon and mean enrichment score\n",
    "    df_shannon = _merge_shannon_enrichment(\n",
    "        self, shannon_entropy, start_position\n",
    "    )\n",
    "\n",
    "    return df_shannon, df_freq\n",
    "\n",
    "\n",
    "def _merge_shannon_enrichment(self, shannon_entropy, start_position):\n",
    "\n",
    "    # Create df with shannon entropy by residue and average enrichment score by residue\n",
    "    df_shannon = pd.DataFrame()\n",
    "    df_shannon['Position'] = np.arange(\n",
    "        start_position,\n",
    "        len(shannon_entropy) + start_position\n",
    "    )\n",
    "    df_shannon['Shannon'] = shannon_entropy\n",
    "\n",
    "    # group by enrichment scores\n",
    "    df_enrichment = self.dataframe.groupby(by='Position', as_index=False).mean()\n",
    "\n",
    "    # Merge Shannon with enrichment scores\n",
    "    df_shannon = df_shannon.merge(df_enrichment, how='inner', on=['Position'])\n",
    "\n",
    "    return df_shannon\n",
    "\n",
    "\n",
    "def _flatten_msa(msa):\n",
    "    """Flatten an msa so each sequence is in one string"""\n",
    "    msa_flattened = []\n",
    "    for sequence in msa:\n",
    "        msa_flattened.append(''.join(sequence))\n",
    "    return msa_flattened\n",
    "\n",
    "\n",
    "def _msa_to_df(msa, correctionfactor=1):\n",
    "    """Convert a msa from a fasta file into a df ready to plot with logomaker. Returns frequency"""\n",
    "    # Flatten MSA\n",
    "    msa_flattened = _flatten_msa(msa)\n",
    "\n",
    "    # Make matrix\n",
    "    df = alignment_to_matrix(msa_flattened)\n",
    "\n",
    "    # Reindex\n",
    "    df.index = np.arange(correctionfactor, len(df) + correctionfactor)\n",
    "\n",
    "    # Return only common aa\n",
    "    aminoacids = list('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "    # Normalize by the total number of counts\n",
    "    df_final = df[aminoacids].copy()\n",
    "\n",
    "    return df_final / df_final.sum(axis=1).max()\n",
    "\n",
    "\n",
    "def _merge_msa_enrichment(self, df_msa, start_position, threshold):\n",
    "    """\n",
    "    Merges msa conservation of each individual amino acid with the \n",
    "    enrichment scores.\n",
    "    \n",
    "    """\n",
    "\n",
    "    # make a dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Create column with position and aminoacid label\n",
    "    df['Position'] = np.ravel(\n",
    "        [[i] * len(df_msa.T)\n",
    "         for i in range(start_position,\n",
    "                        len(df_msa) + start_position)]\n",
    "    )\n",
    "    df['Aminoacid'] = list(df_msa.columns) * len(df_msa)\n",
    "\n",
    "    # Add conservation from MSA\n",
    "    df['Conservation'] = list(df_msa.stack(dropna=False))\n",
    "\n",
    "    # Merge with enrichment scores\n",
    "    df_merged = self.dataframe.merge(\n",
    "        df, how='inner', on=['Position', 'Aminoacid']\n",
    "    )\n",
    "\n",
    "    # Copycat conservation\n",
    "    df_merged['Class'] = df_merged['Conservation']\n",
    "\n",
    "    # Binarize conservation scores. 0 means not conserved\n",
    "    df_merged.loc[df_merged['Conservation'] > threshold, 'Class'] = 1\n",
    "    df_merged.loc[df_merged['Conservation'] <= threshold, 'Class'] = 0\n",
    "    return df_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "100px",
    "left": "21px",
    "top": "69.2px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.4,
   "position": {
    "height": "423.4px",
    "left": "944.6px",
    "right": "20px",
    "top": "119px",
    "width": "315.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
